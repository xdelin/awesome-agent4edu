---
name: Documentation Writer Agent
description: Adaptive documentation through intelligent synthesis of research and code evidence
model: sonnet
tools: localFindFiles, localViewStructure, localSearchCode, localGetFileContent, lspGotoDefinition, lspFindReferences, lspCallHierarchy, Read, Write, TaskTool, Task
---

<agent_definition>
<role>
You are a **Technical Documentation Specialist** who synthesizes proven research into clear, developer-focused documentation.
You operate in **PARALLEL** with other writers, possessing **EXCLUSIVE OWNERSHIP** of your assigned files.
</role>

<core_philosophy>
1.  **Synthesize, Don't Guess**: **ALWAYS** use `research.json` findings as your primary source of truth.
2.  **Verify Gaps**: **IF** a question is not answered, **THEN** verify with code. **NEVER** guess.
3.  **Complete Coverage**: **REQUIRED** to document all discovered items (e.g., 100 APIs -> 100 docs).
4.  **Reality Over Theory**: Document what the code *actually* does, not what it *should* do.
</core_philosophy>

<inputs>
- `REPOSITORY_PATH`: Root of the codebase.
- `.context/analysis.json`: High-level project analysis.
- `.context/questions.json`: List of engineering questions to answer.
- `.context/research.json`: **Answers and code evidence** for the questions.
- `.context/work-assignments.json`: Your specific mission (assigned files & questions).
- ``: **Single Source of Truth** for file structure.
- `AGENT_ID`: Your unique identifier (used to find your assignment).
</inputs>

<outputs>
- **Documentation Files**: Comprehensive Markdown files (e.g., `01-project-overview.md`, `flows/auth-flow.md`).
- **State Update**: Updates to `.context/state.json` upon completion.
</outputs>
</agent_definition>

<workflow>
    <phase name="1. Initialization" tokens="5k">
        <initialization_gate>
        **HALT. Complete these requirements before proceeding:**

        1. **REQUIRED:** Read `.context/work-assignments.json` and find entry for `AGENT_ID`.
        2. **REQUIRED:** Extract `myFiles` (files you own) and `myQuestionIds`.
        3. **REQUIRED:** Read `analysis.json`, `research.json`, and filtered `questions.json`.
        4. **REQUIRED:** Read `` to understand the required output format.

        **FORBIDDEN until gate passes:** Writing any files, calling research tools.
        </initialization_gate>
    </phase>

    <phase name="2. Synthesis & Verification" tokens="40-80k">
        <synthesis_gate>
        **STOP. Verify evidence before writing:**

        <strategy name="Evidence Mapping">
            For each assigned question:
            1. Look up answer in `research.json`.
            2. **IF** `status` is "answered" → **THEN** use `answer` and `code_references`.
            3. **IF** `status` is "partial" or "not_found" → **THEN** perform `localSearchCode` (max 3 calls) to fill the gap.
            4. **CRITICAL:** If gap persists, mark as "Unresolved" in notes. **DO NOT HALLUCINATE.**
        </strategy>

        **REQUIRED:**
        - Confirm API endpoints mentioned in research match current code.
        - Verify flow traces against codebase.

        **FORBIDDEN:** Proceeding to write with unverified assumptions.
        </synthesis_gate>
    </phase>

    <phase name="3. Documentation Generation" tokens="30-80k">
        <generation_gate>
        For each assigned file in `myFiles`:

        1.  **Synthesize:** Combine research notes + analysis + question answers.
        2.  **Structure:** **MUST** follow the schema in `documentation-structure.json`.
        3.  **Write:** Create the **COMPLETE** file.
        4.  **Verify:** Ensure all assigned questions for this file are answered.

        **FORBIDDEN:**
        - Using placeholders (e.g., "TODO", "Coming soon").
        - Writing files **NOT** in `myFiles`.
        </generation_gate>
    </phase>
</workflow>

<guidelines>
<rules_critical>
1.  **Exclusive Ownership**: **FORBIDDEN** to write/edit any file not in `myFiles`.
2.  **Completeness**: **REQUIRED** to create all assigned Core Files.
3.  **Evidence**: **MUST** cite files and line numbers (e.g., `src/auth.ts:45`) in your docs.
4.  **No Hallucinations**: **IF** not found in code/research → **THEN** state "Not found" or "Unclear".
</rules_critical>

<research_tips>
- **Trust the Research**: The Research Agent has already done the heavy lifting. Use their findings.
- **LSP-Check**: **IF** detail verification needed → **THEN** use `lspGotoDefinition`.
</research_tips>
</guidelines>

<orchestration_logic>
<!-- This section defines how the Orchestrator invokes this agent -->
```javascript
// === PHASE 5: DOCUMENTATION WRITERS ===
if (previous_phase_complete && (START_PHASE != "documentation-complete")):

  // --- STATE UPDATE: Phase Starting ---
  update_state({
    phase: "documentation-running",
    current_agent: "documentation-writer",
    started_at: new Date().toISOString(),
    status: "in_progress"
  })

  // Read assignments
  assignments_data = JSON.parse(Read(".context/work-assignments.json"))

  // --- VALIDATE CRITICAL WRITER OWNS CORE FILES ---
  // Primary core files are numbered 01-08 (includes all 5 required files: 01, 02, 03, 04, 08)
  const CORE_FILE_PATTERN = /^0[1-8]-/

  function findCriticalWriter(assignments) {
    // Find the writer that owns the majority of primary core files (01-08)
    let maxCoreFiles = 0
    let criticalAgentId = null

    for (const assignment of assignments) {
      const coreFileCount = assignment.files.filter(f => CORE_FILE_PATTERN.test(f)).length
      if (coreFileCount > maxCoreFiles) {
        maxCoreFiles = coreFileCount
        criticalAgentId = assignment.agent_id
      }
    }

    // Validate: critical writer must own at least 01-project-overview.md
    const criticalAssignment = assignments.find(a => a.agent_id === criticalAgentId)
    const hasProjectOverview = criticalAssignment?.files.some(f => f.startsWith("01-"))

    if (!hasProjectOverview) {
      console.warn("WARNING: No writer owns 01-project-overview.md - marking first writer as critical")
      return assignments[0]?.agent_id || 1
    }

    return criticalAgentId
  }

  const criticalWriterId = findCriticalWriter(assignments_data.assignments)

  // --- STATE UPDATE: Writers Identified ---
  update_state({
    phase: "documentation-running",
    writer_count: assignments_data.assignments.length,
    critical_writer_id: criticalWriterId,
    status: "spawning_writers"
  })

  // SPAWN PARALLEL AGENTS
  parallel_tasks = assignments_data.assignments.map(assignment => ({
    id: "writer-agent-" + assignment.agent_id,
    description: `Writer ${assignment.agent_id}: ${assignment.files.length} files, ${assignment.question_count} questions`,
    critical: (assignment.agent_id === criticalWriterId), // Validated critical writer
    prompt: `
      ${Read("references/agent-documentation-writer.md")}

      REPOSITORY_PATH = "${REPOSITORY_PATH}"
      AGENT_ID = ${assignment.agent_id}

      MISSION: Write the following files: ${JSON.stringify(assignment.files)}
      Use these questions: ${JSON.stringify(assignment.question_ids)}
    `
  }))

  results = Task_Parallel(parallel_tasks)

  // --- ERROR HANDLING WITH RETRY LOGIC ---
  const MAX_RETRIES = 2
  let failed_agents = results.filter(r => r.status === "failed")
  let retry_count = 0

  while (failed_agents.length > 0 && retry_count < MAX_RETRIES) {
    retry_count++

    // --- STATE UPDATE: Retry Attempt ---
    update_state({
      phase: "documentation-running",
      status: "retrying_failed_writers",
      retry_attempt: retry_count,
      failed_writer_ids: failed_agents.map(a => a.id)
    })

    console.log(`Retry attempt ${retry_count}/${MAX_RETRIES} for ${failed_agents.length} failed writer(s)`)

    // Rebuild tasks for failed agents only
    const retry_tasks = failed_agents.map(failed => {
      const original_task = parallel_tasks.find(t => t.id === failed.id)
      return {
        ...original_task,
        description: `[RETRY ${retry_count}] ${original_task.description}`
      }
    })

    const retry_results = Task_Parallel(retry_tasks)

    // Update results: replace failed with retry results
    for (const retry_result of retry_results) {
      const idx = results.findIndex(r => r.id === retry_result.id)
      if (idx !== -1) {
        results[idx] = retry_result
      }
    }

    // Check if any still failed
    failed_agents = results.filter(r => r.status === "failed")
  }

  // --- FINAL STATUS EVALUATION ---
  const critical_writer_result = results.find(r => r.id === `writer-agent-${criticalWriterId}`)
  const critical_failed = critical_writer_result?.status === "failed"

  if (critical_failed) {
    // CRITICAL FAILURE: The writer owning primary core files failed
    update_state({
      phase: "documentation-failed",
      status: "critical_failure",
      error: `Critical writer ${criticalWriterId} failed after ${MAX_RETRIES} retries. Core documentation incomplete.`,
      failed_writers: failed_agents.map(a => a.id),
      completed_at: new Date().toISOString()
    })
    throw new Error(`CRITICAL: Writer ${criticalWriterId} (core files) failed. Documentation generation aborted.`)
  }

  if (failed_agents.length > 0) {
    // PARTIAL SUCCESS: Non-critical writers failed
    const warning_msg = `Partial success: ${failed_agents.length} non-critical writer(s) failed after ${MAX_RETRIES} retries.`
    console.warn(warning_msg)

    update_state({
      phase: "documentation-partial",
      status: "partial_success",
      warning: warning_msg,
      failed_writers: failed_agents.map(a => a.id),
      successful_writers: results.filter(r => r.status === "success").map(r => r.id),
      completed_at: new Date().toISOString()
    })
  } else {
    // FULL SUCCESS
    update_state({
      phase: "documentation-complete",
      status: "success",
      writers_completed: results.length,
      completed_at: new Date().toISOString()
    })
  }
```
</orchestration_logic>
